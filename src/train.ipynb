{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-13T18:11:59.989232Z",
     "start_time": "2024-09-13T18:11:59.611627Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T18:25:04.567484Z",
     "start_time": "2024-09-13T18:25:00.198852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prepare data sets and tokenize\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files={'train': 'train.csv', 'test': 'test.csv'})\n",
    "print(dataset['train'][12])\n",
    "\n",
    "# AST parsing\n",
    "import ast\n",
    "module = ast.parse(dataset['train'][1].get('code_snippet'))\n",
    "module = ast.parse(dataset['train'][35].get('code_snippet'))\n",
    "\n",
    "# CodeBERT tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    code_snippet = examples['code_snippet']\n",
    "    return tokenizer(code_snippet, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ],
   "id": "8b4fb4aa5f610446",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code_snippet': 'import networkx as nx\\\\nimport random\\\\n\\\\ndef randomized_maximal_independent_set(G):\\\\n    independent_set = set()\\\\n    nodes = list(G.nodes())\\\\n    random.shuffle(nodes)\\\\n    while nodes:\\\\n        node = nodes.pop(0)\\\\n        independent_set.add(node)\\\\n        for neighbor in G.neighbors(node):\\\\n            if neighbor in nodes:\\\\n                nodes.remove(neighbor)\\\\n    return independent_set\\\\n\\\\nG = nx.Graph()\\\\nG.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 0), (0, 2)])\\\\nindependent_set = randomized_maximal_independent_set(G)\\\\nprint(f\"Maximal Independent Set: {independent_set}\")', 'labels': 1}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T07:08:28.294517Z",
     "start_time": "2024-09-14T07:08:28.277251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming start_position and end_position are provided\n",
    "start_position = 12\n",
    "end_position = 25\n",
    "\n",
    "# Sample code snippet\n",
    "code_snippet = \"\"\"\n",
    "import networkx as nx\n",
    "\n",
    "def kcoloring_networkx_greedy(G, k):\n",
    "    colors = {}\n",
    "    for node in G.nodes():\n",
    "        available_colors = set(range(k)) - {colors[neighbor] for neighbor in G.neighbors(node) if neighbor in colors}\n",
    "        if available_colors:\n",
    "            colors[node] = min(available_colors)\n",
    "    return colors\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 0)])\n",
    "kcoloring_networkx_greedy(G, 3)\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the code snippet\n",
    "inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Extract the tokens based on start and end positions\n",
    "token_ids = inputs['input_ids'][0]  # Get the token IDs from the input\n",
    "extracted_token_ids = token_ids[start_position:end_position + 1]  # Slice the token sequence\n",
    "\n",
    "# Convert the token IDs back to tokens (strings)\n",
    "extracted_tokens = tokenizer.convert_ids_to_tokens(extracted_token_ids)\n",
    "\n",
    "# Reconstruct the matrix (or code portion) by converting tokens back to string\n",
    "extracted_code = tokenizer.convert_tokens_to_string(extracted_tokens)\n",
    "\n",
    "# Output the extracted code\n",
    "print(f\"Extracted portion of the code: {extracted_code}\")"
   ],
   "id": "5601bce7655e3cf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted portion of the code: coloring_networkx_greedy(G, k):\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T18:14:27.970423Z",
     "start_time": "2024-09-13T18:14:27.967703Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8cfc3fccff752604",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T18:14:32.479450Z",
     "start_time": "2024-09-13T18:14:31.975103Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3c7293e1f30b7d7d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T18:14:34.301317Z",
     "start_time": "2024-09-13T18:14:34.271104Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e577f0b970a20c66",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# Move input and labels to the device\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m(device)\n\u001B[1;32m      9\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     10\u001B[0m     labels \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T18:25:00.196990Z",
     "start_time": "2024-09-13T18:14:54.130738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('csv', data_files={'train': 'train.csv', 'test': 'test.csv'})\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    code_snippet = examples['code_snippet']\n",
    "    return tokenizer(code_snippet, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Specify the fields in the dataset that need to be included\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Define DataLoader\n",
    "train_loader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\n",
    "eval_loader = DataLoader(tokenized_datasets['test'], batch_size=8)\n",
    "\n",
    "# Initialize the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=4)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Move input and labels to the device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Predictions\n",
    "        _, predicted = torch.max(outputs.logits, dim=-1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "id": "7a32e2eec600849a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3306392431259155\n",
      "Epoch 2/10, Loss: 1.2337381442387898\n",
      "Epoch 3/10, Loss: 1.0924411863088608\n",
      "Epoch 4/10, Loss: 0.6948065931598345\n",
      "Epoch 5/10, Loss: 0.439535287519296\n",
      "Epoch 6/10, Loss: 0.259065297121803\n",
      "Epoch 7/10, Loss: 0.16744354863961539\n",
      "Epoch 8/10, Loss: 0.15336805302649736\n",
      "Epoch 9/10, Loss: 0.11567062015334766\n",
      "Epoch 10/10, Loss: 0.11325222564240296\n",
      "Accuracy: 95.56%\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
